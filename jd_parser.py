# -*- coding: utf-8 -*-
"""JD_parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/158BuPcxdAUsjxAHrx2_gJ4drF1k0fgai
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U -q 'google-genai>=1.0.0'

sample_jd = """ Key job responsibilities

  Support the development of continuously-evolving business analytics and data models, own the quantitative analysis of the performance of our sales team, customers, deal team, partners, markets, and products/services in context of private pricing.
 Continually develop new ways of using data to look around the corners of the AWS Private Pricing business
 Use machine learning, data mining, and statistical techniques to design/run experiments that solve complex business problems.
 Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation.
 Develop a deep understanding of sales metrics, reporting tools, and data structures in order to identify and drive resolution of issues, provide actionable intelligence with existing metrics or identify, develop, and propose new metrics, dashboards, scorecards or new tools.
 Develop relationships and processes with sales, finance, sales operations, and other functional teams to identify and address reporting issues.
 Manage and develop advanced analytical tools that align, and simplify, monthly business reviews, annual planning, and forecasting processes.
 Create operational templates and processes to compile and standardize disparate information that drive standardized reporting and metrics tracking.
 Generate ad-hoc and monthly operational analysis and reports, based on the needs of the stakeholders.

Basic Qualifications

  5+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience
 4+ years of data scientist experience
 Experience with statistical models e.g. multinomial logistic regression
 Ability to communicate complex and nuanced messaging to a diverse audience of senior leadership stakeholders

Preferred Qualifications

  2+ years of data visualization using AWS QuickSight, Tableau, R Shiny, etc. experience
 Experience managing data pipelines
 Experience as a leader and mentor on a data science team """

"""# Chat"""

from google.colab import userdata

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')

from google import genai
from google.genai import types

client = genai.Client(api_key=GOOGLE_API_KEY)
MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

system_instruction_recruiter = """
  You are a technical recruiter with 10 years of experience working for tech companies hiring in the areas of Machine Learning and Data Science.
  Given the job description and requirements, your job is to help choose between two resumes - first one being for Data Scientist, second one for Machine Learning Engineer if either is a good match.
  The response should be of json type with parameters like why each resume could be a good fit, what is missing, score on 1-10 and winner i.e which resume is better 1st or the 2nd either is a good fit.
  Make sure winner field in json is either 1 or 2 if either resume works or 0 if neither is a good match
"""
system_prompt_qa = """ Act as a Q&A bot on Varsha' resumes to help fill job applications. Be very clear and say "I do not know" if you do not know the anser or cant back up with solid proof from the given resume pdfs"""
chat_config = types.GenerateContentConfig(
    system_instruction=system_instruction_recruiter,
)

chat = client.chats.create(
    model=MODEL_ID,
    config=chat_config,
)

from google import genai
from google.genai import types
import pathlib
import httpx
from IPython.display import Markdown

client = genai.Client(api_key=GOOGLE_API_KEY)

# Retrieve and encode the PDF byte
filepath_MLE = pathlib.Path('/content/Varsha Vattikonda - MLE.pdf')
filepath_DS = pathlib.Path('/content/Varsha Vattikonda - DS.pdf')

client = genai.Client(api_key=GOOGLE_API_KEY)

response = chat.send_message([
      types.Part.from_bytes(
        data=filepath_DS.read_bytes(),
        mime_type='application/pdf',
      ),
      types.Part.from_bytes(
        data=filepath_MLE.read_bytes(),
        mime_type='application/pdf',
      ),
      "What is the candidate's name"])

Markdown(response.text)
# print(response.text)

response = chat.send_message([
      types.Part.from_bytes(
        data=filepath_DS.read_bytes(),
        mime_type='application/pdf',
      ),
      types.Part.from_bytes(
        data=filepath_MLE.read_bytes(),
        mime_type='application/pdf',
      ),
      """We’re looking for a Machine Learning Engineer to join Snap Inc!

What you’ll do:

Create models which help drive value for users, advertisers, and our company
Evaluate the technical tradeoffs of every decision
Perform code reviews and ensure exceptional code quality
Build robust, lasting, and scalable products Iterate quickly without compromising quality

Knowledge, Skills & Abilities:

Strong understanding of machine learning approaches and algorithms
Able to prioritize duties and work well on your own
Ability to work with both internal and external partners
Skilled at solving open ambiguous problems
Strong collaboration and mentorship skills

Minimum Qualifications:

Bachelor's Degree in a relevant technical field such as computer science or equivalent years of practical work experience
3+ years of post-Bachelor’s machine learning experience; or Master’s degree in a technical field + 2+ year of post-grad machine learning experience; or PhD in a relevant technical field + 1 years of post-grad machine learning experience
Experience developing machine learning models for ranking, recommendations, search, content understanding, image generation, or other relevant applications of machine learning

Preferred Qualifications:

Advanced degree in computer science or related field
Experience working with machine learning frameworks such as TensorFlow, Caffe2, PyTorch, Spark ML, scikit-learn, or related frameworks
Experience working with machine learning, ranking infrastructures, and system design"""])

Markdown(response.text)

system_instruction_recruiter = """
  Help compose a message to the hiring team while applying for a job with the provided job description
"""
system_prompt_qa = """ Act as a Q&A bot on Varsha' resumes to help fill job applications. Be very clear and say "I do not know" if you do not know the anser or cant back up with solid proof from the given resume pdfs"""
chat_config = types.GenerateContentConfig(
    system_instruction=system_instruction_recruiter,
)

chat = client.chats.create(
    model=MODEL_ID,
    config=chat_config,
)

response = chat.send_message([
      types.Part.from_bytes(
        data=filepath_DS.read_bytes(),
        mime_type='application/pdf',
      ),
      types.Part.from_bytes(
        data=filepath_MLE.read_bytes(),
        mime_type='application/pdf',
      ),
      sample_jd])

Markdown(response.text)

"""# Q & A from Resume"""

!pip install langchain_google_genai
!pip install langchain_openai
!pip install langchain_community
!pip install pypdf
!pip install faiss-cpu

from google.colab import userdata
google_api_key = userdata.get('GOOGLE_API_KEY')

import os
os.environ['GOOGLE_API_KEY'] = google_api_key
# Use DirectoryLoader for loading multiple documents
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA




MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash", "gemini-2.5-pro"] {"allow-input":true, isTemplate: true}

# --- 1. LOAD DOCUMENTS FROM A DIRECTORY ---
print("Loading resumes from directory...")
# We specify the directory and the glob pattern to find all .pdf files
# We also specify the PyPDFLoader to be used for each file
loader = DirectoryLoader(
    path="/content/",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader
)
documents = loader.load()
print(f"Loaded {len(documents)} documents.")

# Optional: You can print the metadata of the first few documents to check the 'source'
# for doc in documents[:2]:
#     print(doc.metadata)


# --- 2. SPLIT DOCUMENTS INTO CHUNKS ---
print("Splitting documents into chunks...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)
# The metadata (including 'source') is automatically carried over to the chunks
print(f"Split into {len(texts)} chunks.")


# --- 3. CREATE EMBEDDINGS AND STORE IN VECTORSTORE ---
print("Creating embeddings and vector store...")
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
vectorstore = FAISS.from_documents(texts, embeddings)
print("Vector store created.")


# --- 4. CREATE THE RAG CHAIN THAT RETURNS SOURCE DOCUMENTS ---
print("Creating RAG chain...")
retriever = vectorstore.as_retriever()
llm = ChatGoogleGenerativeAI(model=MODEL_ID, temperature=0, convert_system_message_to_human=True)

# We use a standard RetrievalQA chain but configure it to return the source documents
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True  # This is the key change!
)
print("RAG chain created. You can now ask questions.")
print("-" * 50)


# --- 5. ASK QUESTIONS AND DISPLAY RESULTS WITH SOURCES ---
def ask_question_with_source(question):
    print(f"\n> Query: {question}")
    # The result is now a dictionary that includes 'source_documents'
    result = qa_chain.invoke({"query": question})

    print(f"  Answer: {result['result']}")

    # Print the source documents
    print("  Sources:")
    for source_doc in result['source_documents']:
        # The 'source' is in the metadata
        print(f"    - {os.path.basename(source_doc.metadata['source'])}")

ask_question_with_source('Linkedin profile link. Is probably embedded in the doc. Try finding it')

